{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # Chemical Named Entity Recognition\n",
    "Chemical NER is the extraction of chemical mentions from the scientific texts such as patents, bio-medical literature, patient records etc and classify them to entities such as trivial names, drugs, abbrevations, molecular formulas and IUPAC entities. In the NER system the key stages are: pre-processing methods, feature-extraction and sequence labelling approaches. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset used is the CHEMDNER corpus which consists of 10,000 MEDLINE titles and abstracts randomly partitioned to training, testing and development datasets. The articles were curated from a list of articles published in 2013 by the top 100 journals related to the chemistry field. These articles are manually annotated by domain experts. Each annotation consists of the article identifier, type of text (title or abstract), start and end indices, the text string and the label of chemical entity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import DataFrame, Series\n",
    "\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from chemtok import ChemTokeniser\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "\n",
    "import string\n",
    "import csv\n",
    "import re\n",
    "\n",
    "import sklearn_crfsuite\n",
    "from sklearn_crfsuite import CRF, scorers, metrics\n",
    "from sklearn_crfsuite.metrics import flat_f1_score, flat_classification_report\n",
    "from sklearn.model_selection import RandomizedSearchCV, cross_val_predict, cross_val_score, learning_curve\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore',category=DeprecationWarning)\n",
    "warnings.filterwarnings('ignore',category=UserWarning)\n",
    "warnings.filterwarnings('ignore',category=FutureWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first import the training data which contains article ID, title & abstracts extracted from 3500 documents and 29478 annotated entities into pandas dataframe. The annotation data contains article ID, entity type, start & end indices of entity in that article, entity name and its label.  The test data contains title and abstracts extracted from 3000 documents with 25352 annotated entities(we consider 1500 documents in the project due to memory constraints). The development data contains title & abstracts extracted from 3500 documents with 29526 annotated entities. The training, test & development data has been annotated with the following entity types: abbreviation (e.g., \"DMSO\"), family (e.g., \"Iodopyridazines\"), formula (e.g., \"(CH3)2SO\"), identifier (e.g., \"CHEBI:28262\"), multiple (e.g., \"thieno2,3-d and thieno3,2-d fused oxazin-4-ones\"), systematic (e.g., \"2-Acetoxybenzoic acid\"), trivial (e.g., \"Aspirin\"), and undefined/no class (e.g., \"C4-C-N-PEG9\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_ID</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21826085</td>\n",
       "      <td>DPP6 as a candidate gene for neuroleptic-induc...</td>\n",
       "      <td>We implemented a two-step approach to detect p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22080034</td>\n",
       "      <td>Nanosilver effects on growth parameters in exp...</td>\n",
       "      <td>Aflatoxicosis is a cause of economic losses in...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22080035</td>\n",
       "      <td>The influence of the intensity of smoking and ...</td>\n",
       "      <td>The aim of this study was to investigate the e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22080037</td>\n",
       "      <td>Mercury induces the expression of cyclooxygena...</td>\n",
       "      <td>Nuclear factor-κB (NF-κB) is a transcription f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22258629</td>\n",
       "      <td>Toxic effects of chromium on tannery workers a...</td>\n",
       "      <td>Chromium is widely used in the leather industr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_ID                                              title  \\\n",
       "0    21826085  DPP6 as a candidate gene for neuroleptic-induc...   \n",
       "1    22080034  Nanosilver effects on growth parameters in exp...   \n",
       "2    22080035  The influence of the intensity of smoking and ...   \n",
       "3    22080037  Mercury induces the expression of cyclooxygena...   \n",
       "4    22258629  Toxic effects of chromium on tannery workers a...   \n",
       "\n",
       "                                            abstract  \n",
       "0  We implemented a two-step approach to detect p...  \n",
       "1  Aflatoxicosis is a cause of economic losses in...  \n",
       "2  The aim of this study was to investigate the e...  \n",
       "3  Nuclear factor-κB (NF-κB) is a transcription f...  \n",
       "4  Chromium is widely used in the leather industr...  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing training, test and development/validation dataset into dataframes\n",
    "df1_train_data = pd.read_csv('training.abstracts.txt', delimiter = \"\\t\", header=None,names=[\"article_ID\", \"title\", \"abstract\"])\n",
    "df2_train_data = pd.read_csv('training.annotations.txt', delimiter = \"\\t\", header=None,names=[\"article_ID\", \"e_type\", \"start\",\"end\",\"entity\",\"label\"])\n",
    "\n",
    "df1_test_data = pd.read_csv('test-abstracts.csv', nrows=1500)\n",
    "df2_test_data = pd.read_csv('test-annt.csv', nrows=12824)\n",
    "\n",
    "df1_dev_data = pd.read_csv('development.abstracts.txt', delimiter = \"\\t\", header=None,names=[\"article_ID\", \"title\", \"abstract\"])\n",
    "df2_dev_data = pd.read_csv('development.annotations.txt', delimiter = \"\\t\", header=None,names=[\"article_ID\", \"e_type\", \"start\",\"end\",\"entity\",\"label\"])\n",
    "df1_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_ID</th>\n",
       "      <th>e_type</th>\n",
       "      <th>start</th>\n",
       "      <th>end</th>\n",
       "      <th>entity</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21826085</td>\n",
       "      <td>A</td>\n",
       "      <td>946</td>\n",
       "      <td>957</td>\n",
       "      <td>haloperidol</td>\n",
       "      <td>TRIVIAL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22080034</td>\n",
       "      <td>A</td>\n",
       "      <td>190</td>\n",
       "      <td>199</td>\n",
       "      <td>aflatoxin</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22080034</td>\n",
       "      <td>A</td>\n",
       "      <td>594</td>\n",
       "      <td>603</td>\n",
       "      <td>aflatoxin</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22080034</td>\n",
       "      <td>A</td>\n",
       "      <td>718</td>\n",
       "      <td>727</td>\n",
       "      <td>aflatoxin</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22080034</td>\n",
       "      <td>A</td>\n",
       "      <td>1072</td>\n",
       "      <td>1081</td>\n",
       "      <td>aflatoxin</td>\n",
       "      <td>FAMILY</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_ID e_type  start   end       entity    label\n",
       "0    21826085      A    946   957  haloperidol  TRIVIAL\n",
       "1    22080034      A    190   199    aflatoxin   FAMILY\n",
       "2    22080034      A    594   603    aflatoxin   FAMILY\n",
       "3    22080034      A    718   727    aflatoxin   FAMILY\n",
       "4    22080034      A   1072  1081    aflatoxin   FAMILY"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next import brown clustering files generated from 500 clusters which will be used in feature extraction stage of the process. The brown clustering algorithm is applied for training, test and development datasets and contains bitchain binary representation of word, the word and the count of number of times the word occur in that dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bitchain_title</th>\n",
       "      <th>word_title</th>\n",
       "      <th>count_title</th>\n",
       "      <th>bitchain_abst</th>\n",
       "      <th>word_abst</th>\n",
       "      <th>count_abst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Chloroaluminium</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>possible),</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>of</td>\n",
       "      <td>3183</td>\n",
       "      <td>0</td>\n",
       "      <td>(1995).</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>\"Absolute</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>14-76%</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>and</td>\n",
       "      <td>2182</td>\n",
       "      <td>0</td>\n",
       "      <td>CHD,</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>β-galactosidase.</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>shRNA-mediated</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bitchain_title        word_title  count_title  bitchain_abst  \\\n",
       "0               0  \"Chloroaluminium            1              0   \n",
       "1               0                of         3183              0   \n",
       "2               0         \"Absolute            1              0   \n",
       "3              10               and         2182              0   \n",
       "4              10  β-galactosidase.            1              0   \n",
       "\n",
       "        word_abst  count_abst  \n",
       "0      possible),           1  \n",
       "1         (1995).           1  \n",
       "2          14-76%           1  \n",
       "3            CHD,           1  \n",
       "4  shRNA-mediated           1  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train_brown = pd.read_csv('title_train_cluster.txt', delimiter = \"\\t\", header=None, encoding='utf-8', names=[\"bitchain_title\", \"word_title\",\"count_title\"])\n",
    "df_train_brown[[\"bitchain_abst\", \"word_abst\",\"count_abst\"]] = pd.read_csv('abstract_train_cluster.txt', delimiter = \"\\t\", header=None, encoding='utf-8', names=[\"bitchain_abst\", \"word_abst\",\"count_abst\"])\n",
    "df_test_brown = pd.read_csv('title_test_cluster.txt', delimiter = \"\\t\", header=None, encoding='utf-8', names=[\"bitchain_title\", \"word_title\",\"count_title\"])\n",
    "df_test_brown[[\"bitchain_abst\", \"word_abst\",\"count_abst\"]] = pd.read_csv('abstract_test_cluster.txt', delimiter = \"\\t\", header=None, encoding='utf-8', names=[\"bitchain_abst\", \"word_abst\",\"count_abst\"])\n",
    "df_dev_brown = pd.read_csv('title_dev_cluster.txt', delimiter = \"\\t\", header=None, encoding='utf-8', quoting=csv.QUOTE_NONE, names=[\"bitchain_title\", \"word_title\",\"count_title\"])\n",
    "df_dev_brown[[\"bitchain_abst\", \"word_abst\",\"count_abst\"]] = pd.read_csv('abstract_dev_cluster.txt',quoting=csv.QUOTE_NONE,delimiter = \"\\t\", header=None, encoding='utf-8', names=[\"bitchain_abst\", \"word_abst\",\"count_abst\"])\n",
    "\n",
    "df_train_brown['bitchain_title'] = df_train_brown['bitchain_title'].apply(lambda x: int(str(x)[:2]))\n",
    "df_train_brown['bitchain_abst'] = df_train_brown['bitchain_abst'].apply(lambda x: int(str(x)[:2]))\n",
    "df_test_brown['bitchain_title'] = df_test_brown['bitchain_title'].apply(lambda x: int(str(x)[:2]))\n",
    "df_test_brown['bitchain_abst'] = df_test_brown['bitchain_abst'].apply(lambda x: int(str(x)[:2]))\n",
    "df_dev_brown['bitchain_title'] = df_dev_brown['bitchain_title'].apply(lambda x: int(str(x)[:2]))\n",
    "df_dev_brown['bitchain_abst'] = df_dev_brown['bitchain_abst'].apply(lambda x: int(str(x)[:2]))\n",
    "df_dev_brown.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We next import CheBI database text files which is later used for dictionary lookup feature in the feature extraction stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>COMPOUND_ID</th>\n",
       "      <th>TYPE</th>\n",
       "      <th>SOURCE</th>\n",
       "      <th>NAME</th>\n",
       "      <th>ADAPTED</th>\n",
       "      <th>LANGUAGE</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10952</td>\n",
       "      <td>16478</td>\n",
       "      <td>SYNONYM</td>\n",
       "      <td>KEGG COMPOUND</td>\n",
       "      <td>N-Acetyl-beta-D-glucosaminyl-1,6-(N-acetyl-bet...</td>\n",
       "      <td>F</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10957</td>\n",
       "      <td>15947</td>\n",
       "      <td>SYNONYM</td>\n",
       "      <td>KEGG COMPOUND</td>\n",
       "      <td>N-Acetyl-beta-D-glucosaminylamine</td>\n",
       "      <td>F</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11955</td>\n",
       "      <td>7853</td>\n",
       "      <td>SYNONYM</td>\n",
       "      <td>KEGG COMPOUND</td>\n",
       "      <td>Oxyacanthine</td>\n",
       "      <td>F</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11966</td>\n",
       "      <td>15379</td>\n",
       "      <td>SYNONYM</td>\n",
       "      <td>KEGG COMPOUND</td>\n",
       "      <td>O2</td>\n",
       "      <td>F</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11965</td>\n",
       "      <td>15379</td>\n",
       "      <td>SYNONYM</td>\n",
       "      <td>KEGG COMPOUND</td>\n",
       "      <td>Oxygen</td>\n",
       "      <td>F</td>\n",
       "      <td>en</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ID  COMPOUND_ID     TYPE         SOURCE  \\\n",
       "0  10952        16478  SYNONYM  KEGG COMPOUND   \n",
       "1  10957        15947  SYNONYM  KEGG COMPOUND   \n",
       "2  11955         7853  SYNONYM  KEGG COMPOUND   \n",
       "3  11966        15379  SYNONYM  KEGG COMPOUND   \n",
       "4  11965        15379  SYNONYM  KEGG COMPOUND   \n",
       "\n",
       "                                                NAME ADAPTED LANGUAGE  \n",
       "0  N-Acetyl-beta-D-glucosaminyl-1,6-(N-acetyl-bet...       F       en  \n",
       "1                  N-Acetyl-beta-D-glucosaminylamine       F       en  \n",
       "2                                       Oxyacanthine       F       en  \n",
       "3                                                 O2       F       en  \n",
       "4                                             Oxygen       F       en  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_cheBI_FORMULA = pd.read_csv('chemical_data.txt', delimiter = \"\\t\", header=None,names=[\"ID\", \"COMPOUND_ID\", \"SOURCE\",\"TYPE\",\"CHEMICAL_DATA\"])\n",
    "df_cheBI_IUPAC = pd.read_csv('chemical_names.txt', delimiter = \"\\t\", header=None,names=[\"ID\", \"COMPOUND_ID\", \"TYPE\",\"SOURCE\",\"NAME\",'ADAPTED','LANGUAGE'])\n",
    "df_cheBI_IUPAC.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pre-processing stage includes tokenization of sentences, stopwords removal, parts of speech tagging and BIO labelling of the tokens. These steps in CNER are crucial and can be challenging in identifying chemical entities, as the chemical compounds and drug names may contain a number of symbols mixed with common words or digits.  \n",
    "\n",
    "###### Tokenization\n",
    "Tokenization is the process of segmenting raw text into words/tokens. The natural language tokenisers can wrongly determine or split the chemical words containing special symbols or braces.\n",
    "For example: \"cyanidin-3-O-[6-O-(4-O-E-caffeoyl-O-α-rhamnopyranosyl)-β-glucopyrano]-5-O-β-glucopyranoside\" is a drug which should be determined as a single token. The tokeniser should recognize such a long intact tokens which is not possible by using natural language tokenizers. To acheive this, we use chemtok module taken from OSCR project which is designed to tokenize chemical texts. \n",
    "\n",
    "###### Stopwords Removal\n",
    "The english basic words such as \"about\", \"all\", \"most\", and \"make\" can be removed from the tokens using nltk.\n",
    "\n",
    "###### POS tagging\n",
    "A Part-Of-Speech Tagger (POS Tagger) is a module that reads text and assigns parts of speech to each word/token, such as noun, verb, adjective, etc\n",
    "\n",
    "We first tokenize the titles of training and test data, followed by the abstracts:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_ID</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21826085</td>\n",
       "      <td>DPP6 as a candidate gene for neuroleptic-induc...</td>\n",
       "      <td>We implemented a two-step approach to detect p...</td>\n",
       "      <td>[DPP6, candidate, gene, neuroleptic, -, induce...</td>\n",
       "      <td>[(DPP6, NNP), (candidate, NN), (gene, NN), (ne...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22080034</td>\n",
       "      <td>Nanosilver effects on growth parameters in exp...</td>\n",
       "      <td>Aflatoxicosis is a cause of economic losses in...</td>\n",
       "      <td>[Nanosilver, effects, growth, parameters, expe...</td>\n",
       "      <td>[(Nanosilver, NNP), (effects, NNS), (growth, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22080035</td>\n",
       "      <td>The influence of the intensity of smoking and ...</td>\n",
       "      <td>The aim of this study was to investigate the e...</td>\n",
       "      <td>[The, influence, intensity, smoking, years, wo...</td>\n",
       "      <td>[(The, DT), (influence, NN), (intensity, NN), ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22080037</td>\n",
       "      <td>Mercury induces the expression of cyclooxygena...</td>\n",
       "      <td>Nuclear factor-κB (NF-κB) is a transcription f...</td>\n",
       "      <td>[Mercury, induces, expression, cyclooxygenase-...</td>\n",
       "      <td>[(Mercury, NNP), (induces, NNS), (expression, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22258629</td>\n",
       "      <td>Toxic effects of chromium on tannery workers a...</td>\n",
       "      <td>Chromium is widely used in the leather industr...</td>\n",
       "      <td>[Toxic, effects, chromium, tannery, workers, S...</td>\n",
       "      <td>[(Toxic, NNP), (effects, NNS), (chromium, VBP)...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_ID                                              title  \\\n",
       "0    21826085  DPP6 as a candidate gene for neuroleptic-induc...   \n",
       "1    22080034  Nanosilver effects on growth parameters in exp...   \n",
       "2    22080035  The influence of the intensity of smoking and ...   \n",
       "3    22080037  Mercury induces the expression of cyclooxygena...   \n",
       "4    22258629  Toxic effects of chromium on tannery workers a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We implemented a two-step approach to detect p...   \n",
       "1  Aflatoxicosis is a cause of economic losses in...   \n",
       "2  The aim of this study was to investigate the e...   \n",
       "3  Nuclear factor-κB (NF-κB) is a transcription f...   \n",
       "4  Chromium is widely used in the leather industr...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  [DPP6, candidate, gene, neuroleptic, -, induce...   \n",
       "1  [Nanosilver, effects, growth, parameters, expe...   \n",
       "2  [The, influence, intensity, smoking, years, wo...   \n",
       "3  [Mercury, induces, expression, cyclooxygenase-...   \n",
       "4  [Toxic, effects, chromium, tannery, workers, S...   \n",
       "\n",
       "                                           title_tag  \n",
       "0  [(DPP6, NNP), (candidate, NN), (gene, NN), (ne...  \n",
       "1  [(Nanosilver, NNP), (effects, NNS), (growth, N...  \n",
       "2  [(The, DT), (influence, NN), (intensity, NN), ...  \n",
       "3  [(Mercury, NNP), (induces, NNS), (expression, ...  \n",
       "4  [(Toxic, NNP), (effects, NNS), (chromium, VBP)...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization, stop words removal and POS tagging of titles from training dataset\n",
    "df1_train_data['title_tokens'] = df1_train_data['title'].apply(lambda x: ChemTokeniser(x, clm=True))\n",
    "df1_train_data['title_tokens'] = df1_train_data['title_tokens'].apply(lambda x: [x.value for x in x.tokens])\n",
    "df1_train_data['title_tokens'] = df1_train_data['title_tokens'].apply(lambda x: [w for w in x if w!=\".\"])\n",
    "df1_train_data['title_tokens'] = df1_train_data['title_tokens'].apply(lambda x: [w for w in x if w not in stopwords.words('english')])\n",
    "df1_train_data['title_tag'] = df1_train_data['title_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "#tokenization, stop words removal and POS tagging of titles from test dataset\n",
    "df1_test_data['title_tokens'] = df1_test_data['title'].apply(lambda x: ChemTokeniser(x, clm=True))\n",
    "df1_test_data['title_tokens'] = df1_test_data['title_tokens'].apply(lambda x: [x.value for x in x.tokens])\n",
    "df1_test_data['title_tokens'] = df1_test_data['title_tokens'].apply(lambda x: [w for w in x if w!=\".\"])\n",
    "df1_test_data['title_tokens'] = df1_test_data['title_tokens'].apply(lambda x: [w for w in x if w not in stopwords.words('english')])\n",
    "df1_test_data['title_tag'] = df1_test_data['title_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "#tokenization, stop words removal and POS tagging of titles from development dataset\n",
    "df1_dev_data['title_tokens'] = df1_dev_data['title'].apply(lambda x: ChemTokeniser(x, clm=True))\n",
    "df1_dev_data['title_tokens'] = df1_dev_data['title_tokens'].apply(lambda x: [x.value for x in x.tokens])\n",
    "df1_dev_data['title_tokens'] = df1_dev_data['title_tokens'].apply(lambda x: [w for w in x if w!=\".\"])\n",
    "df1_dev_data['title_tokens'] = df1_dev_data['title_tokens'].apply(lambda x: [w for w in x if w not in stopwords.words('english')])\n",
    "df1_dev_data['title_tag'] = df1_dev_data['title_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "df1_train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_ID</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_tokens</th>\n",
       "      <th>title_tag</th>\n",
       "      <th>abstract_tokens</th>\n",
       "      <th>abstract_tag</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>21826085</td>\n",
       "      <td>DPP6 as a candidate gene for neuroleptic-induc...</td>\n",
       "      <td>We implemented a two-step approach to detect p...</td>\n",
       "      <td>[DPP6, candidate, gene, neuroleptic, -, induce...</td>\n",
       "      <td>[(DPP6, NNP), (candidate, NN), (gene, NN), (ne...</td>\n",
       "      <td>[We, implemented, two, -, step, approach, dete...</td>\n",
       "      <td>[(We, PRP), (implemented, VBD), (two, CD), (-,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>22080034</td>\n",
       "      <td>Nanosilver effects on growth parameters in exp...</td>\n",
       "      <td>Aflatoxicosis is a cause of economic losses in...</td>\n",
       "      <td>[Nanosilver, effects, growth, parameters, expe...</td>\n",
       "      <td>[(Nanosilver, NNP), (effects, NNS), (growth, N...</td>\n",
       "      <td>[Aflatoxicosis, cause, economic, losses, broil...</td>\n",
       "      <td>[(Aflatoxicosis, NNP), (cause, VBP), (economic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>22080035</td>\n",
       "      <td>The influence of the intensity of smoking and ...</td>\n",
       "      <td>The aim of this study was to investigate the e...</td>\n",
       "      <td>[The, influence, intensity, smoking, years, wo...</td>\n",
       "      <td>[(The, DT), (influence, NN), (intensity, NN), ...</td>\n",
       "      <td>[The, aim, study, investigate, effect, cigaret...</td>\n",
       "      <td>[(The, DT), (aim, NN), (study, NN), (investiga...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>22080037</td>\n",
       "      <td>Mercury induces the expression of cyclooxygena...</td>\n",
       "      <td>Nuclear factor-κB (NF-κB) is a transcription f...</td>\n",
       "      <td>[Mercury, induces, expression, cyclooxygenase-...</td>\n",
       "      <td>[(Mercury, NNP), (induces, NNS), (expression, ...</td>\n",
       "      <td>[Nuclear, factor-κB, (, NF-κB, ), transcriptio...</td>\n",
       "      <td>[(Nuclear, JJ), (factor-κB, JJ), ((, (), (NF-κ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>22258629</td>\n",
       "      <td>Toxic effects of chromium on tannery workers a...</td>\n",
       "      <td>Chromium is widely used in the leather industr...</td>\n",
       "      <td>[Toxic, effects, chromium, tannery, workers, S...</td>\n",
       "      <td>[(Toxic, NNP), (effects, NNS), (chromium, VBP)...</td>\n",
       "      <td>[Chromium, widely, used, leather, industry, ,,...</td>\n",
       "      <td>[(Chromium, NNP), (widely, RB), (used, VBD), (...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_ID                                              title  \\\n",
       "0    21826085  DPP6 as a candidate gene for neuroleptic-induc...   \n",
       "1    22080034  Nanosilver effects on growth parameters in exp...   \n",
       "2    22080035  The influence of the intensity of smoking and ...   \n",
       "3    22080037  Mercury induces the expression of cyclooxygena...   \n",
       "4    22258629  Toxic effects of chromium on tannery workers a...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  We implemented a two-step approach to detect p...   \n",
       "1  Aflatoxicosis is a cause of economic losses in...   \n",
       "2  The aim of this study was to investigate the e...   \n",
       "3  Nuclear factor-κB (NF-κB) is a transcription f...   \n",
       "4  Chromium is widely used in the leather industr...   \n",
       "\n",
       "                                        title_tokens  \\\n",
       "0  [DPP6, candidate, gene, neuroleptic, -, induce...   \n",
       "1  [Nanosilver, effects, growth, parameters, expe...   \n",
       "2  [The, influence, intensity, smoking, years, wo...   \n",
       "3  [Mercury, induces, expression, cyclooxygenase-...   \n",
       "4  [Toxic, effects, chromium, tannery, workers, S...   \n",
       "\n",
       "                                           title_tag  \\\n",
       "0  [(DPP6, NNP), (candidate, NN), (gene, NN), (ne...   \n",
       "1  [(Nanosilver, NNP), (effects, NNS), (growth, N...   \n",
       "2  [(The, DT), (influence, NN), (intensity, NN), ...   \n",
       "3  [(Mercury, NNP), (induces, NNS), (expression, ...   \n",
       "4  [(Toxic, NNP), (effects, NNS), (chromium, VBP)...   \n",
       "\n",
       "                                     abstract_tokens  \\\n",
       "0  [We, implemented, two, -, step, approach, dete...   \n",
       "1  [Aflatoxicosis, cause, economic, losses, broil...   \n",
       "2  [The, aim, study, investigate, effect, cigaret...   \n",
       "3  [Nuclear, factor-κB, (, NF-κB, ), transcriptio...   \n",
       "4  [Chromium, widely, used, leather, industry, ,,...   \n",
       "\n",
       "                                        abstract_tag  \n",
       "0  [(We, PRP), (implemented, VBD), (two, CD), (-,...  \n",
       "1  [(Aflatoxicosis, NNP), (cause, VBP), (economic...  \n",
       "2  [(The, DT), (aim, NN), (study, NN), (investiga...  \n",
       "3  [(Nuclear, JJ), (factor-κB, JJ), ((, (), (NF-κ...  \n",
       "4  [(Chromium, NNP), (widely, RB), (used, VBD), (...  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#tokenization, stop words removal and POS tagging of abstracts from training dataset\n",
    "df1_train_data['abstract_tokens'] = df1_train_data['abstract'].apply(lambda x: ChemTokeniser(x, clm=True))\n",
    "df1_train_data['abstract_tokens'] = df1_train_data['abstract_tokens'].apply(lambda x: [x.value for x in x.tokens])\n",
    "df1_train_data['abstract_tokens'] = df1_train_data['abstract_tokens'].apply(lambda x: [w for w in x if w!=\".\"])\n",
    "df1_train_data['abstract_tokens'] = df1_train_data['abstract_tokens'].apply(lambda x: [w for w in x if w not in stopwords.words('english')])\n",
    "df1_train_data['abstract_tag'] = df1_train_data['abstract_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "#tokenization, stop words removal and POS tagging of abstracts from test dataset\n",
    "df1_test_data['abstract_tokens'] = df1_test_data['abstract'].apply(lambda x: ChemTokeniser(x, clm=True))\n",
    "df1_test_data['abstract_tokens'] = df1_test_data['abstract_tokens'].apply(lambda x: [x.value for x in x.tokens])\n",
    "df1_test_data['abstract_tokens'] = df1_test_data['abstract_tokens'].apply(lambda x: [w for w in x if w!=\".\"])\n",
    "df1_test_data['abstract_tokens'] = df1_test_data['abstract_tokens'].apply(lambda x: [w for w in x if w not in stopwords.words('english')])\n",
    "df1_test_data['abstract_tag'] = df1_test_data['abstract_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "#tokenization, stop words removal and POS tagging of abstracts from development dataset\n",
    "df1_dev_data['abstract_tokens'] = df1_dev_data['abstract'].apply(lambda x: ChemTokeniser(x, clm=True))\n",
    "df1_dev_data['abstract_tokens'] = df1_dev_data['abstract_tokens'].apply(lambda x: [x.value for x in x.tokens])\n",
    "df1_dev_data['abstract_tokens'] = df1_dev_data['abstract_tokens'].apply(lambda x: [w for w in x if w!=\".\"])\n",
    "df1_dev_data['abstract_tokens'] = df1_dev_data['abstract_tokens'].apply(lambda x: [w for w in x if w not in stopwords.words('english')])\n",
    "df1_dev_data['abstract_tag'] = df1_dev_data['abstract_tokens'].apply(lambda x: nltk.pos_tag(x))\n",
    "\n",
    "df1_train_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BIO Tagging\n",
    "The BIO format is a common tagging format for tagging tokens in a NER system. The \"B-\" prefix before a tag indicates that the tag is the beginning of a chunk, and an \"I-\" prefix before a tag indicates that the tag is inside a chunk. The \"B-\" tag is used only when a tag is followed by a tag of the same type without O tokens between them. An \"O\" tag indicates that a token belongs to no entity / chunk.\n",
    "\n",
    "E.g.,: \"Mercury induces the expression of cyclooxygenase-2 and inducible nitric oxide synthase\"<br>\n",
    "BIO tag: Mercury/B/induces/O/expression/O/cyclooxygenase-2/O/inducible/O/nitric/B/Oxide/I/synthase/O"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BIO tagging of title and abstracts\n",
    "def mark_sentences_bio(df1,df2):\n",
    "    doc_t, doc_a = [],[]\n",
    "    tit,tagged_t,bitchain_t = [],[],[]\n",
    "    #BIO tagging of abstracts\n",
    "    for article_id,title,title_tok,title_tag in zip(df1.article_ID,df1.title,df1.title_tokens,df1.title_tag):\n",
    "        word_dict = {}\n",
    "        bit_dict_t = {}\n",
    "        for word in title_tok:\n",
    "            word_dict[word] = 'O'               \n",
    "        for art_id,start,end,label in zip(df2.article_ID,df2.start,df2.end,df2.label):\n",
    "            if(article_id==art_id):\n",
    "                temp_str = title[int(start):int(end)]\n",
    "                temp_list = temp_str.split()\n",
    "                if len(temp_list) > 1:\n",
    "                    word_dict[temp_list[0]] = 'B-' + label\n",
    "                    for w in temp_list[1:]:\n",
    "                        word_dict[w] = 'I-' + label\n",
    "                else: \n",
    "                    word_dict[temp_str] = 'B-' + label\n",
    "                if temp_str not in title_tok and temp_str!='':\n",
    "                    title_tag.append((temp_str,''))\n",
    "        tit.append([(k,v) for k, v in word_dict.items() if k!=''])\n",
    "        tagged_t.append(title_tag)\n",
    "    #assigning POS and entity label to each word\n",
    "    for  label, pos in zip(tit, tagged_t):\n",
    "        doc_t.append([(w, pos1, label1) for (w,label1),(word,pos1) in zip(label,pos)])\n",
    "                                                \n",
    "    abst,tagged_a,bitchain_a = [],[],[]\n",
    "    #BIO tagging of abstracts\n",
    "    for art_id,abstract,abstract_tok,abstract_tag in zip(df1.article_ID,df1.abstract,df1.abstract_tokens,df1.abstract_tag):\n",
    "        word_dict = {}\n",
    "        bit_dict_a = {}\n",
    "        for w in abstract_tok:\n",
    "            word_dict[w] = 'O'                 \n",
    "        for ar_id,start,end,label in zip(df2.article_ID,df2.start,df2.end,df2.label):\n",
    "            if(art_id==ar_id):\n",
    "                temp_str = abstract[int(start):int(end)]\n",
    "                temp_list = temp_str.split()\n",
    "                if len(temp_list) > 1:\n",
    "                    word_dict[temp_list[0]] = 'B-' + label\n",
    "                    for w in temp_list[1:]:\n",
    "                        word_dict[w] = 'I-' + label\n",
    "                else: \n",
    "                    word_dict[temp_str] = 'B-' + label\n",
    "                if temp_str not in abstract_tok and temp_str!='':\n",
    "                    abstract_tag.append((temp_str,''))\n",
    "        abst.append([(k, v) for k, v in word_dict.items()])\n",
    "        tagged_a.append(abstract_tag)\n",
    "    #assigning POS and entity label to each word\n",
    "    for  label, pos in zip(abst, tagged_a):\n",
    "        doc_a.append([(w, pos1, label1) for (w,label1),(word,pos1) in zip(label,pos)])\n",
    "    return doc_t,doc_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mercury', 'NNP', 'B-SYSTEMATIC'), ('induces', 'NNS', 'O'), ('expression', 'VBP', 'O'), ('cyclooxygenase-2', 'JJ', 'O'), ('inducible', 'JJ', 'O'), ('nitric', 'JJ', 'B-SYSTEMATIC'), ('oxide', 'NN', 'I-SYSTEMATIC'), ('synthase', 'NN', 'O')]\n"
     ]
    }
   ],
   "source": [
    "#BIO tagging of training dataset\n",
    "res_t = mark_sentences_bio(df1_train_data,df2_train_data)\n",
    "print(res_t[0][3])\n",
    "\n",
    "#BIO tagging of test dataset\n",
    "res_a = mark_sentences_bio(df1_test_data,df2_test_data)\n",
    "\n",
    "#BIO tagging of development dataset\n",
    "res_d = mark_sentences_bio(df1_dev_data,df2_dev_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature Extraction\n",
    "\n",
    "In the Feature extraction stage, the feature space is being represented in which different types of features are being used to generate the characteristics of the tokens/words. This is the most critical stage of NER, as the machine learning model can be better trained using different word features and representations. We use different categories of feature extraction such as morphological features, orthographical features, chemical dictionary lookup and unsupervised word representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Morphological features:\n",
    "Reflects common structures and sub-sequences of characters among entities. Such as, prefixes, suffixes, \n",
    "char n-grams:bigrams,trigrams, word-shape patterns(e.g.,the structure \"Abc:1234\" can be expressed as \"Aaa#1111\")\n",
    "\n",
    "#### Orthographical features:\n",
    "Represents word formation information, such as, capital letters, length of the word, ContainsDigits, ContainsPunctutaion, ContainsRoman, ContainsGreek\n",
    "\n",
    "#### Chemical dictionary lookup:\n",
    "Recognising that the occurrence of a token in an expert-curated dictionary indicates a high likelihood of it being a chemical name constituent. Dictionaries of chemicals are used to match the tokens/words. We have used Chemical Entities of Biological Interest (ChEBI) database to match chemical formulas and names.\n",
    "\n",
    "#### Unsupervised word representation\n",
    "If new terms show up in the test cases that have not been seen previously in the training dataset, we should adopt unsupervised WR like clustering for such words. We use two types of clustering approaches that are used widely in recognizing chemical entities:<br>\n",
    "\n",
    "**Brown Clustering**: Brown clustering builds a hierarchical cluster for words according to the context similarity\n",
    "among those words. The hierarchical path of a word in the cluster can be used as the word representation feature.\n",
    "We followed the method in (https://github.com/percyliang/brown-cluster/) to generate Brown clustering features. The hierarchical clusters are represented by a binary tree and words that are semantically/\n",
    "syntactically similar are assumed to be in the same or close clusters and have similar feature representations. For example, both of the words, oxygen and nitrogen are represented as ‘110110110110’ from the hierarchical binary\n",
    "tree generated from 500 Brown clusters.\n",
    "\n",
    "**Word Embeddings**: Word embeddings induce a real valued latent syntactic/semantic vector for each word from large corpus by continuous space language models. A word can be directly represented by its vector and similar words are likely to have similar vectors. A word2vec statistical neural network model is used to generate standalone word embeddings. Two different learning models can be used as part of the word2vec approach to learn the word embeddings they are:<br>\n",
    "* The CBOW model: learns the embedding by predicting the current word based on its context.<br>\n",
    "* The skip-gram model: learns by predicting the surrounding words given a current word.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#assigning bitchain binary representation for each word in the dataset\n",
    "def get_bitchain(df1,df2,res):\n",
    "    bitchain_t,bitchain_a,data = [],[],[]\n",
    "    for doc_t,doc_a in zip(res[0],res[1]):\n",
    "        bitchain_dict_t, bitchain_dict_a= {},{}\n",
    "        for word in doc_t:\n",
    "            bitchain_title_train = df2['bitchain_title'].loc[df2['word_title'] == word[0]].values[-1:]\n",
    "            if (len(bitchain_title_train)==0): bitchain = \"\"\n",
    "            else:             \n",
    "                for k in range(len(bitchain_title_train)):\n",
    "                    bitchain = bitchain_title_train[-k-1]\n",
    "            bitchain_dict_t[word[0]] = bitchain\n",
    "        #assigning bitchain for each word in title\n",
    "        bitchain_t.append([(l,m) for l,m in bitchain_dict_t.items()])\n",
    "        \n",
    "        for w in doc_a:\n",
    "            bitchain_abst_train = df2['bitchain_abst'].loc[df2['word_abst'] == w[0]].values[-1:]\n",
    "            if (len(bitchain_abst_train)==0): bitchain = \"\"\n",
    "            else:             \n",
    "                for k in range(len(bitchain_abst_train)):\n",
    "                    bitchain = bitchain_abst_train[-k-1]\n",
    "            bitchain_dict_a[w[0]] = bitchain           \n",
    "        bitchain_a.append([(l,m) for l,m in bitchain_dict_a.items()]) \n",
    "    #assigning bitchain for each word in abstract\n",
    "    for doc,bitchain in zip(res[0],bitchain_t):\n",
    "        data.append([(w, pos1, label1,bit) for (w, pos1, label1),(word,bit) in zip(doc,bitchain)])\n",
    "    for doc,bitchain in zip(res[1],bitchain_a):\n",
    "        data.append([(w, pos1, label1,bit) for (w, pos1, label1),(word,bit) in zip(doc,bitchain)])\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Mercury', 'NNP', 'B-SYSTEMATIC', ''), ('induces', 'NNS', 'O', 10), ('expression', 'VBP', 'O', 11), ('cyclooxygenase-2', 'JJ', 'O', 10), ('inducible', 'JJ', 'O', 10), ('nitric', 'JJ', 'B-SYSTEMATIC', 11), ('oxide', 'NN', 'I-SYSTEMATIC', 11), ('synthase', 'NN', 'O', 11)]\n"
     ]
    }
   ],
   "source": [
    "#assigning bitchain for each token in training data\n",
    "data_train = get_bitchain(df1_train_data,df_train_brown,res_t)\n",
    "print(data_train[3])\n",
    "\n",
    "#assigning bitchain for each token in test data\n",
    "data_test = get_bitchain(df1_test_data,df_test_brown,res_a)\n",
    "\n",
    "#assigning bitchain for each token in development data\n",
    "data_dev = get_bitchain(df1_dev_data,df_dev_brown,res_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating word embeddings using word2vec model for each tokens\n",
    "df_token_data = []\n",
    "for index,row in df1_train_data.iterrows():\n",
    "    df_token_data.append(row['title_tokens'])\n",
    "    df_token_data.append(row['abstract_tokens'])\n",
    "for index,row in df1_test_data.iterrows():\n",
    "    df_token_data.append(row['title_tokens'])\n",
    "    df_token_data.append(row['abstract_tokens'])\n",
    "    \n",
    "model = Word2Vec(sentences=df_token_data,vector_size=100, window=5, min_count=1, workers=4)\n",
    "model.save('model.bin')\n",
    "\n",
    "#load and train the model\n",
    "model = Word2Vec.load('model.bin')\n",
    "model.train(df_token_data, total_examples=1, epochs=1)\n",
    "word_vectors = model.wv\n",
    "\n",
    "#save the model\n",
    "word_vectors.save(\"word2vec.wordvectors\")\n",
    "\n",
    "#load the model in keyedVectors format which occupies less data \n",
    "wv = KeyedVectors.load(\"word2vec.wordvectors\", mmap='r')\n",
    "def get_features(word):\n",
    "    word=word.lower()\n",
    "    try:\n",
    "        #word embeddings of each word/token is represented in vector\n",
    "         vector=wv[word]\n",
    "    except:\n",
    "        # if the word is not in vocabulary,\n",
    "        # returns zeros array\n",
    "        vector=np.zeros(300,dtype='uint8')\n",
    "    return vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating features of word which can be fed to train the model. The crf model used in our project accepts features in dictionary format as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating morphological, orthographic, dictionary lookup, wordembedding, brown cluster features for word/token\n",
    "def word2features(doc, i):\n",
    "    word = doc[i][0]\n",
    "    postag = doc[i][1]\n",
    "    bitchain = doc[i][3]\n",
    " \n",
    "    punct = set(string.punctuation)\n",
    "    #common prefixes used in chemical data\n",
    "    pref = [\"alk\", \"meth\", \"eth\", \"prop\", \"but\", \"pent\", \"hex\", \"hept\", \"oct\", \"non\", \"dec\", \"undec\", \"dodec\", \"eifcos\",\n",
    "            \"di\", \"tri\", \"tetra\", \"penta\", \"hexa\", \"hepta\"]\n",
    "    #common suffixes used in chemical data\n",
    "    suff = [\"ane\", \"ene\", \"yne\", \"yl\", \"ol\", \"al\", \"oic\", \"one\", \"ate\", \"amine\", \"amide\"]\n",
    "    \n",
    "    #bigrams of word\n",
    "    bi_g = [word[w:w+2] for w in range(len(word)-1)]\n",
    "    #trigrams of word\n",
    "    tri_g = [word[w:w+3] for w in range(len(word)-2)]\n",
    "    \n",
    "    #word pattern\n",
    "    pattern = re.sub(r\"[A-Z]\",\"A\",word)\n",
    "    pattern = re.sub(r\"[a-z]\",\"a\",pattern)\n",
    "    pattern = re.sub(r\"[0-9]\",\"0\",pattern)\n",
    "    pattern = re.sub(r\"[!\"\"#$%&'()*+,./:;<=>?@\\^_`{|}~-]\",\"_\",pattern)\n",
    "    pattern = re.sub(r\"(.)\\1+\", r\"\\1\",pattern)\n",
    "    \n",
    "    #wordembedding vector of each word\n",
    "    wordembedding=get_features(word)\n",
    "    #chemical database lookup\n",
    "    isinChEBIFormula = word in df_cheBI_FORMULA.CHEMICAL_DATA\n",
    "    isinChEBIIUPAC = word in df_cheBI_IUPAC.NAME\n",
    "\n",
    "    # Common features for all words\n",
    "    features = {\n",
    "        'bias':1.0,\n",
    "        'word.lower()' : word.lower(),\n",
    "        'word.islower()' : word.islower(),\n",
    "        'word[-3:]': word[-3:],\n",
    "        'word[-2:]': word[-2:],\n",
    "        'word.isupper()': word.isupper(),\n",
    "        'word.istitle()': word.istitle(),\n",
    "        'word.isdigit()': word.isdigit(),        \n",
    "        'word[0].isupper()' : word[0:].isupper(),\n",
    "        'word.length' : len(word),\n",
    "        'word.punctuation' : (any(char in punct for char in word)),\n",
    "        'word.prefix' : (any(word.startswith(pf) for pf in pref)),\n",
    "        'word.suffix' : (any(word.endswith(sf) for sf in suff)),\n",
    "        'word.greek' : (bool(re.search('[α-ωΑ-Ω]',word))),\n",
    "        'word.roman' : (bool(re.search(r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\",word))),\n",
    "        'word.pattern' : pattern,\n",
    "        'word.bitchain': bitchain,\n",
    "        'word.isinChEBIFormula': isinChEBIFormula,\n",
    "        'word:isinChEBIIUPAC' : isinChEBIIUPAC,\n",
    "        'postag' : postag,\n",
    "        'postag[:2]': postag[:2]   \n",
    "    }\n",
    "    for j in range(len(bi_g)):\n",
    "        features['bigram_{}'.format(j+1)]=bi_g[j]\n",
    "    for j in range(len(tri_g)):\n",
    "        features['trigram_{}'.format(j+1)]=tri_g[j]    \n",
    "    for iv,value in enumerate(wordembedding):\n",
    "        features['v{}'.format(iv)]=value    \n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the beginning of a document\n",
    "    if i > 0:\n",
    "        word1 = doc[i-1][0]\n",
    "        postag1 = doc[i-1][1]\n",
    "        bitchain1 = doc[i-1][3]\n",
    "        pattern1 = re.sub(r\"[A-Z]\",\"A\",word1)\n",
    "        pattern1 = re.sub(r\"[a-z]\",\"a\",pattern1)\n",
    "        pattern1 = re.sub(r\"[0-9]\",\"0\",pattern1)\n",
    "        pattern1 = re.sub(r\"[!\"\"#$%&'()*+,./:;<=>?@\\^_`{|}~-]\",\"_\",pattern1)\n",
    "        pattern1 = re.sub(r\"(.)\\1+\", r\"\\1\",pattern1)      \n",
    "        bi_g1 = [word1[w:w+2] for w in range(len(word1)-1)]\n",
    "        tri_g1 = [word1[w:w+3] for w in range(len(word1)-2)]  \n",
    "        wordembedding1=get_features(word1)  \n",
    "        isinChEBIFormula1 = word1 in df_cheBI_FORMULA.CHEMICAL_DATA\n",
    "        isinChEBIIUPAC1 = word1 in df_cheBI_IUPAC.NAME\n",
    "        features.update({\n",
    "            '-1:word.lower()' : word1.lower(),\n",
    "            '-1:word.islower()' : word1.islower(),\n",
    "            '-1:word[-3:]': word1[-3:],\n",
    "            '-1:word[-2:]': word1[-2:],\n",
    "            '-1:word.isupper()': word1.isupper(),\n",
    "            '-1:word.istitle()': word1.istitle(),\n",
    "            '-1:word.isdigit()': word1.isdigit(),        \n",
    "            '-1:word[0].isupper()' : word1[0:].isupper(),\n",
    "            '-1:word.length' : len(word1),\n",
    "            '-1:word.punctuation' : (any(char in punct for char in word1)),\n",
    "            '-1:word.prefix' : (any(word1.startswith(pf) for pf in pref)),\n",
    "            '-1:word.suffix' : (any(word1.endswith(sf) for sf in suff)),\n",
    "            '-1:word.greek' : (bool(re.search('[α-ωΑ-Ω]',word1))),\n",
    "            '-1:word.roman' : (bool(re.search(r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\",word1))),\n",
    "            '-1:postag' : postag1,         \n",
    "            '-1:word.pattern' : pattern1,  \n",
    "            '-1:word.bitchain': bitchain1,\n",
    "            '-1:word.isinChEBIFormula': isinChEBIFormula1,\n",
    "            '-1:word:isinChEBIIUPAC' : isinChEBIIUPAC1,\n",
    "            '-1:postag[:2]': postag1[:2]\n",
    "        })\n",
    "        for j in range(len(bi_g1)):\n",
    "            features['-1:bigram_{}'.format(j+1)]=bi_g1[j]\n",
    "        for j in range(len(tri_g1)):\n",
    "            features['-1:trigram_{}'.format(j+1)]=tri_g1[j]  \n",
    "        for iv,value in enumerate(wordembedding1):\n",
    "            features['-1:v{}'.format(iv)]=value\n",
    "    else:\n",
    "        # Indicate that it is the 'beginning of a document'\n",
    "        features['BOS'] = True\n",
    "\n",
    "    # Features for words that are not\n",
    "    # at the end of a document\n",
    "    if i < len(doc)-1:\n",
    "        word1 = doc[i+1][0]\n",
    "        postag1 = doc[i+1][1]\n",
    "        bitchain1 = doc[i+1][3]\n",
    "\n",
    "        pattern1 = re.sub(r\"[A-Z]\",\"A\",word1)\n",
    "        pattern1 = re.sub(r\"[a-z]\",\"a\",pattern1)\n",
    "        pattern1 = re.sub(r\"[0-9]\",\"0\",pattern1)\n",
    "        pattern1 = re.sub(r\"[!\"\"#$%&'()*+,./:;<=>?@\\^_`{|}~-]\",\"_\",pattern1)\n",
    "        pattern1 = re.sub(r\"(.)\\1+\", r\"\\1\",pattern1)      \n",
    "        bi_g1 = [word1[w:w+2] for w in range(len(word1)-1)]\n",
    "        tri_g1 = [word1[w:w+3] for w in range(len(word1)-2)]  \n",
    "        wordembedding1=get_features(word1)        \n",
    "        isinChEBIFormula1 = word1 in df_cheBI_FORMULA.CHEMICAL_DATA\n",
    "        isinChEBIIUPAC1 = word1 in df_cheBI_IUPAC.NAME\n",
    "        features.update({\n",
    "            '+1:word.lower()' : word1.lower(),\n",
    "            '+1:word.islower()' : word1.islower(),\n",
    "            '+1:word[-3:]': word1[-3:],\n",
    "            '+1:word[-2:]': word1[-2:],\n",
    "            '+1:word.isupper()': word1.isupper(),\n",
    "            '+1:word.istitle()': word1.istitle(),\n",
    "            '+1:word.isdigit()': word1.isdigit(),        \n",
    "            '+1:word[0].isupper()' : word1[0:].isupper(),\n",
    "            '+1:word.length' : len(word1),\n",
    "            '+1:word.punctuation' : (any(char in punct for char in word1)),\n",
    "            '+1:word.prefix' : (any(word1.startswith(pf) for pf in pref)),\n",
    "            '+1:word.suffix' : (any(word1.endswith(sf) for sf in suff)),\n",
    "            '+1:word.greek' : (bool(re.search('[α-ωΑ-Ω]',word1))),\n",
    "            '+1:word.roman' : (bool(re.search(r\"^M{0,3}(CM|CD|D?C{0,3})(XC|XL|L?X{0,3})(IX|IV|V?I{0,3})$\",word1))),            \n",
    "            '+1:word.pattern' : pattern1,\n",
    "            '+1:word.isinChEBIFormula': isinChEBIFormula1,\n",
    "            '+1:word:isinChEBIIUPAC' : isinChEBIIUPAC1,\n",
    "            '+1:postag' : postag1,\n",
    "            '+1:postag[:2]': postag1[:2]\n",
    "        })\n",
    "        for i in range(len(bi_g1)):\n",
    "            features['+1:bigram_{}'.format(i+1)]=bi_g1[i]\n",
    "        for i in range(len(tri_g1)):\n",
    "            features['+1:trigram_{}'.format(i+1)]=tri_g1[i]  \n",
    "        for iv,value in enumerate(wordembedding1):\n",
    "            features['+1:v{}'.format(iv)]=value                  \n",
    "\n",
    "    else:\n",
    "        # Indicate that it is the 'end of a document'\n",
    "        features['EOS'] = True\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_features(doc):\n",
    "    return [word2features(doc, i) for i in range(len(doc))]\n",
    "def get_labels(doc):\n",
    "    return [label for (token, postag, label,bitchain) in doc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating word features for training data\n",
    "X_train = [extract_features(doc) for doc in data_train]\n",
    "y_train = [get_labels(doc) for doc in data_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating word features for test data\n",
    "X_test = [extract_features(doc) for doc in data_test]\n",
    "y_test = [get_labels(doc) for doc in data_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating word features for development data\n",
    "X_dev = [extract_features(doc) for doc in data_dev]\n",
    "y_dev = [get_labels(doc) for doc in data_dev]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training CRF Model\n",
    "In this stage, the training data can be used to train a CRF (conditional random fields) model using the extracted features. CRFs are graphical model which implements sequential dependencies in the predictions, i.e., the model predicts many variables that are interdependent. The main challenge in the NER problem is that the too rare entities can occur in test set due to which model must identify based only on context. The naive approach to this problem is to classify each word independently which assumes that named entity labels are independent which is not the case.<br><br>\n",
    "For example: tenofovir is classified as TRIVIAL name, where as tenofovir monophosphate is classified as SYSTEMATIC name.<br><br>\n",
    "To tackle this problem we use CRFs where input data is sequence and output is also a sequence and we have to take the previous context into account when predicting on a data point. For this purpose, we will use a feature functions that will have multiple input values.<br><br>\n",
    "\n",
    "The feature function is defined as follows:<br>\n",
    "       &emsp;&emsp;&emsp;&emsp;f(X,i,Yi-1,Yi) <br>\n",
    "Where, X = set of input vectors<br>\n",
    "       i = position of data points that we want to predict<br>\n",
    "       Y(i-1) = the label of data point i-1 in X<br>\n",
    "       Yi = the label of data point i in X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#model training of training dataset\n",
    "crf = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True)\n",
    "crf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-SYSTEMATIC',\n",
       " 'I-SYSTEMATIC',\n",
       " 'B-FAMILY',\n",
       " 'I-FAMILY',\n",
       " 'B-ABBREVIATION',\n",
       " 'B-TRIVIAL',\n",
       " 'I-TRIVIAL',\n",
       " 'I-MULTIPLE',\n",
       " 'B-MULTIPLE',\n",
       " 'B-IDENTIFIER',\n",
       " 'B-FORMULA',\n",
       " 'I-FORMULA',\n",
       " 'I-ABBREVIATION',\n",
       " 'I-IDENTIFIER',\n",
       " 'B-NO CLASS',\n",
       " 'I-NO CLASS']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#eliminating entities which are not chemical mentions and are represented by 'O' tags\n",
    "labels = list(crf.classes_)\n",
    "labels.remove('O')\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the Model Performance\n",
    "We will use precision, recall and f1-score metrics to evaluate the performance of the model. The recall, precision and f-measure are defined as follows.<br><br>\n",
    "recall=TP/TP+FN, &emsp;&emsp; precision=TP/TP+FP,&emsp;&emsp; f-measure=(2 * precision * recall)/(precision+recall)<br><br>\n",
    "Where,True positive (TP) refers to the number of correctly recognized chemical mentions. False negative (FN) is the number of annotated chemical mentions that were omitted by the presented system. False positive (FP) is the number of recognized chemical mentions that were not annotated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.551359721015118"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predicting entity recognition of test data\n",
    "y_pred = crf.predict(X_test)\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted',labels=labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following results shows the precision, recall & f-measure of recognizing entities of test data when we train the crf model using training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "B-ABBREVIATION       0.81      0.49      0.61       952\n",
      "I-ABBREVIATION       0.00      0.00      0.00        20\n",
      "      B-FAMILY       0.63      0.47      0.54      1840\n",
      "      I-FAMILY       0.38      0.27      0.31      1136\n",
      "     B-FORMULA       0.78      0.69      0.73       954\n",
      "     I-FORMULA       0.14      0.05      0.08        55\n",
      "  B-IDENTIFIER       0.74      0.21      0.33       163\n",
      "  I-IDENTIFIER       0.00      0.00      0.00        33\n",
      "    B-MULTIPLE       0.25      0.07      0.10        91\n",
      "    I-MULTIPLE       0.46      0.21      0.28       228\n",
      "    B-NO CLASS       0.00      0.00      0.00        11\n",
      "    I-NO CLASS       0.00      0.00      0.00         2\n",
      "  B-SYSTEMATIC       0.69      0.65      0.67      2694\n",
      "  I-SYSTEMATIC       0.43      0.35      0.39      1474\n",
      "     B-TRIVIAL       0.74      0.63      0.68      3142\n",
      "     I-TRIVIAL       0.44      0.28      0.34      1054\n",
      "\n",
      "     micro avg       0.64      0.50      0.56     13849\n",
      "     macro avg       0.41      0.27      0.32     13849\n",
      "  weighted avg       0.62      0.50      0.55     13849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now train the crf development/validation dataset to improve the quality of the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CRF(algorithm='lbfgs', all_possible_transitions=True, c1=0.1, c2=0.1,\n",
       "    keep_tempfiles=None, max_iterations=100)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf_dev = CRF(\n",
    "    algorithm='lbfgs', \n",
    "    c1=0.1, \n",
    "    c2=0.1, \n",
    "    max_iterations=100, \n",
    "    all_possible_transitions=True)\n",
    "crf_dev.fit(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['B-SYSTEMATIC',\n",
       " 'I-SYSTEMATIC',\n",
       " 'I-FAMILY',\n",
       " 'B-FAMILY',\n",
       " 'B-TRIVIAL',\n",
       " 'I-TRIVIAL',\n",
       " 'B-FORMULA',\n",
       " 'I-FORMULA',\n",
       " 'B-ABBREVIATION',\n",
       " 'B-IDENTIFIER',\n",
       " 'I-IDENTIFIER',\n",
       " 'B-NO CLASS',\n",
       " 'I-MULTIPLE',\n",
       " 'B-MULTIPLE',\n",
       " 'I-ABBREVIATION',\n",
       " 'I-NO CLASS']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dev = list(crf_dev.classes_)\n",
    "labels_dev.remove('O')\n",
    "labels_dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Hyperparameter Optimization\n",
    "To perform optimization, we can do the regularization of parameters c1 and c2 of the model using randomized search and 3-fold cross-validation for 5 iterations(upto 50 iterations can also be performed given better CPU time and memory limit) and fit the development/validation dataset for the optimized parameters. Then we evaluate the model on the test data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  15 out of  15 | elapsed: 138.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 2h 27min 17s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomizedSearchCV(cv=3,\n",
       "                   estimator=CRF(algorithm='lbfgs',\n",
       "                                 all_possible_transitions=True,\n",
       "                                 keep_tempfiles=None, max_iterations=100),\n",
       "                   n_iter=5, n_jobs=1,\n",
       "                   param_distributions={'c1': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000028B6BD736D0>,\n",
       "                                        'c2': <scipy.stats._distn_infrastructure.rv_frozen object at 0x0000028B6BD73280>},\n",
       "                   scoring=make_scorer(flat_f1_score, average=weighted, labels=['B-SYSTEMATIC', 'I-SYSTEMATIC', 'I-FAMILY', 'B-FAMILY', 'B-TRIVIAL', 'I-TRIVIAL', 'B-FORMULA', 'I-FORMULA', 'B-ABBREVIATION', 'B-IDENTIFIER', 'I-IDENTIFIER', 'B-NO CLASS', 'I-MULTIPLE', 'B-MULTIPLE', 'I-ABBREVIATION', 'I-NO CLASS']),\n",
       "                   verbose=1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "import scipy.stats\n",
    "from itertools import chain\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "crf = sklearn_crfsuite.CRF(\n",
    "    algorithm='lbfgs',\n",
    "    max_iterations=100,\n",
    "    all_possible_transitions=True\n",
    ")\n",
    "\n",
    "params_space = {\n",
    "    'c1': scipy.stats.expon(scale=0.5),\n",
    "    'c2': scipy.stats.expon(scale=0.05),\n",
    "}\n",
    "\n",
    "\n",
    "f1_scorer = make_scorer(metrics.flat_f1_score,\n",
    "                        average='weighted', labels=labels_dev)\n",
    "\n",
    "rs = RandomizedSearchCV(crf, params_space,\n",
    "                        cv=3,\n",
    "                        verbose=1,\n",
    "                        n_jobs=1,\n",
    "                        n_iter=5,\n",
    "                        scoring=f1_scorer)\n",
    "rs.fit(X_dev, y_dev)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best params: {'c1': 0.08894140215067371, 'c2': 0.0020242384494808985}\n",
      "best CV score: 0.512885774771498\n",
      "model size: 5.60M\n"
     ]
    }
   ],
   "source": [
    "print('best params:', rs.best_params_)\n",
    "print('best CV score:', rs.best_score_)\n",
    "print('model size: {:0.2f}M'.format(rs.best_estimator_.size_ / 1000000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking best estimator on our test data. The f1 score and precision, recall score are improved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9925771828887452"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "crf = rs.best_estimator_\n",
    "y_pred = crf.predict(X_test)\n",
    "\n",
    "metrics.flat_f1_score(y_test, y_pred,\n",
    "                      average='weighted',labels=labels_dev)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe from the below results that training & optimizing the model with the development/validation data gave better results in recognizing the chemical entities of test data than the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "B-ABBREVIATION       1.00      1.00      1.00       952\n",
      "I-ABBREVIATION       1.00      1.00      1.00        20\n",
      "      B-FAMILY       1.00      0.99      0.99      1840\n",
      "      I-FAMILY       0.99      0.97      0.98      1136\n",
      "     B-FORMULA       1.00      1.00      1.00       954\n",
      "     I-FORMULA       1.00      1.00      1.00        55\n",
      "  B-IDENTIFIER       1.00      0.99      0.99       163\n",
      "  I-IDENTIFIER       1.00      1.00      1.00        33\n",
      "    B-MULTIPLE       1.00      1.00      1.00        91\n",
      "    I-MULTIPLE       1.00      1.00      1.00       228\n",
      "    B-NO CLASS       1.00      1.00      1.00        11\n",
      "    I-NO CLASS       1.00      1.00      1.00         2\n",
      "  B-SYSTEMATIC       1.00      0.99      1.00      2694\n",
      "  I-SYSTEMATIC       0.99      0.97      0.98      1474\n",
      "     B-TRIVIAL       1.00      0.99      1.00      3142\n",
      "     I-TRIVIAL       1.00      0.97      0.98      1054\n",
      "\n",
      "     micro avg       1.00      0.99      0.99     13849\n",
      "     macro avg       1.00      0.99      1.00     13849\n",
      "  weighted avg       1.00      0.99      0.99     13849\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sorted_labels = sorted(\n",
    "    labels_dev,\n",
    "    key=lambda name: (name[1:], name[0])\n",
    ")\n",
    "print(metrics.flat_classification_report(\n",
    "    y_test, y_pred, labels=sorted_labels, digits=2\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below shows what the classifier learned. We can see that, for example, it is very likely that the beginning of an identifier name (B-IDENTIFIER) will be followed by a token inside identifier name (I-IDENTIFIER)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top likely transitions:\n",
      "B-NO CLASS -> I-NO CLASS 6.722555\n",
      "B-IDENTIFIER -> I-IDENTIFIER 6.414588\n",
      "B-ABBREVIATION -> I-ABBREVIATION 5.908931\n",
      "B-FAMILY -> I-FAMILY 5.195519\n",
      "B-MULTIPLE -> I-MULTIPLE 5.169787\n",
      "B-FORMULA -> I-FORMULA 4.823790\n",
      "I-MULTIPLE -> I-MULTIPLE 4.794898\n",
      "B-SYSTEMATIC -> I-SYSTEMATIC 4.629315\n",
      "B-TRIVIAL -> I-TRIVIAL 4.628872\n",
      "B-FORMULA -> B-FORMULA 3.637423\n",
      "I-FAMILY -> I-FAMILY 3.486416\n",
      "I-SYSTEMATIC -> I-SYSTEMATIC 3.412703\n",
      "B-NO CLASS -> B-NO CLASS 3.306423\n",
      "I-FORMULA -> I-FORMULA 3.281422\n",
      "I-IDENTIFIER -> I-IDENTIFIER 3.101716\n",
      "I-TRIVIAL -> I-TRIVIAL 2.496813\n",
      "O      -> O       2.037894\n",
      "B-ABBREVIATION -> B-ABBREVIATION 1.882757\n",
      "I-TRIVIAL -> B-ABBREVIATION 1.539906\n",
      "I-FORMULA -> B-FORMULA 1.519302\n",
      "\n",
      "Top unlikely transitions:\n",
      "B-MULTIPLE -> B-FORMULA -1.433482\n",
      "I-SYSTEMATIC -> I-IDENTIFIER -1.466668\n",
      "I-FORMULA -> B-SYSTEMATIC -1.473499\n",
      "B-SYSTEMATIC -> I-MULTIPLE -1.536675\n",
      "B-FAMILY -> I-IDENTIFIER -1.576991\n",
      "I-MULTIPLE -> I-TRIVIAL -1.578228\n",
      "B-TRIVIAL -> I-FAMILY -1.589689\n",
      "O      -> I-IDENTIFIER -1.605138\n",
      "I-SYSTEMATIC -> I-FORMULA -1.628555\n",
      "I-TRIVIAL -> I-FORMULA -1.650067\n",
      "B-ABBREVIATION -> I-TRIVIAL -1.684800\n",
      "I-FAMILY -> B-IDENTIFIER -1.705607\n",
      "I-ABBREVIATION -> O       -1.726509\n",
      "I-FORMULA -> I-TRIVIAL -1.918594\n",
      "B-SYSTEMATIC -> I-FORMULA -1.964673\n",
      "B-SYSTEMATIC -> B-MULTIPLE -1.991946\n",
      "B-MULTIPLE -> B-SYSTEMATIC -2.286559\n",
      "I-SYSTEMATIC -> I-TRIVIAL -2.288761\n",
      "O      -> I-FORMULA -2.357459\n",
      "B-TRIVIAL -> I-FORMULA -2.374980\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def print_transitions(trans_features):\n",
    "    for (label_from, label_to), weight in trans_features:\n",
    "        print(\"%-6s -> %-7s %0.6f\" % (label_from, label_to, weight))\n",
    "\n",
    "print(\"Top likely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common(20))\n",
    "\n",
    "print(\"\\nTop unlikely transitions:\")\n",
    "print_transitions(Counter(crf.transition_features_).most_common()[-20:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The performance of the model can be evaluated using a learning curve, plotting the accuracy/score of predictions on the development and test datasets. As more data is fed for training the model, the accuracy on the test data increases with increase in the number of training samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n",
    "                        n_jobs=1, train_sizes=np.linspace(.1, 1.0, 5)):\n",
    "    plt.figure()\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training samples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1,\n",
    "                     color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n",
    "             label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n",
    "             label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    return plt\n",
    "\n",
    "estimator=crf_dev\n",
    "title = \"Learning curves CRF\"\n",
    "\n",
    "plot_learning_curve(estimator, title, X_dev, y_dev,(0.7, 1.01), cv=3, n_jobs=1)\n",
    "\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
